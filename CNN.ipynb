{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.7 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "b8300e87f33265e9d7628c4e66a31d43913b3c3894f5b42d3fae4d1c5a62f323"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as python_random\n",
    "\n",
    "# The below is necessary for starting Numpy generated random numbers\n",
    "np.random.seed(13)\n",
    "# The below is necessary for starting core Python generated random numbers\n",
    "python_random.seed(13)\n",
    "\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPool2D, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow import random as tf_random\n",
    "\n",
    "# The below set_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "tf_random.set_seed(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import glob \n",
    "import skimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"UrbanSound8K.csv\")\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat1, sampling_rate1 = librosa.load('./fold5/100032-3-0-0.wav')\n",
    "arr = librosa.feature.melspectrogram(y=dat1, sr=sampling_rate1)\n",
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = []\n",
    "label = []\n",
    "\n",
    "# Veri setini işlenebilir hale getiren fonksiyon\n",
    "def parser(row):\n",
    "    for i in range(df.shape[0]):\n",
    "        file_name = './fold' + str(df[\"fold\"][i]) + '/' + df[\"slice_file_name\"][i]\n",
    "        # Hızlı yüklemek için kaiser_fast kullanıldı.\n",
    "        X, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
    "        # MFCC Feature. Sütunların aritmetik ortalaması ile flatten array oluşturuldu.\n",
    "        mels = np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate).T,axis=0)        \n",
    "        feature.append(mels)\n",
    "        label.append(df[\"classID\"][i])\n",
    "\n",
    "    print(str(df.shape[0]) + \" extraction completed.\")\n",
    "    return [feature, label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = parser(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.array(temp)\n",
    "data = temp.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data[:, 0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = data[:, 0]\n",
    "Y = data[:, 1]\n",
    "print(X_.shape, Y.shape)\n",
    "X = np.empty([8732, len(data[:, 0][0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(X_.shape[0]):\n",
    "    X[i] = (X_[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = to_categorical(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataseti train ve test olmak üzere 2'ye ayırıyor. Random_state seed atayarak\n",
    "#her seferinde aynı şekilde bölmeyi sağlıyor. Stratify'da her çıktı class'ını\n",
    "#oranlayarak bölmeyi sağlıyor.\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state = 13, stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input_dim\n",
    "num_rows = 16\n",
    "num_columns = 8\n",
    "num_channels = 1\n",
    "\n",
    "#128 boyutunda olan veriyi CNN kullanmak amacı ile 16,8,1 boyutlarında reshape yapıyor.\n",
    "X_train = X_train.reshape(6549, num_rows, num_columns, num_channels)\n",
    "X_test = X_test.reshape(2183, num_rows, num_columns, num_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = X_train.shape[1]\n",
    "num_columns = X_train.shape[2]\n",
    "num_channels = X_train.shape[3]\n",
    "input_dim = (num_rows, num_columns, num_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model oluştuluyor.\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Layerlar ekleniyor.\n",
    "model.add(Conv2D(64, (3, 3), padding = \"same\", activation = \"relu\", input_shape = input_dim))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(128, (3, 3), padding = \"same\", activation = \"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation = \"relu\"))\n",
    "model.add(Dense(10, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train, epochs = 100, batch_size = 128, validation_data = (X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy graph\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#loss graph\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy\n",
    "predictions = model.predict(X_test)\n",
    "score_train = model.evaluate(X_train, Y_train)\n",
    "score_test = model.evaluate(X_test, Y_test)\n",
    "print(score_train)\n",
    "print(score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "preds = np.argmax(predictions, axis = 1)\n",
    "cm = confusion_matrix(Y_test.argmax(axis=1), preds)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, xticklabels=[\"air_conditioner\",\"car_horn\",\"children_playing\",\"dog_bark\",\"drilling\",\"engine_idling\",\"gun_shot\",\"jackhammer\",\"siren\",\"street_music\"], yticklabels=[\"air_conditioner\",\"car_horn\",\"children_playing\",\"dog_bark\",\"drilling\",\"engine_idling\",\"gun_shot\",\"jackhammer\",\"siren\",\"street_music\"], annot=True, fmt='g')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}